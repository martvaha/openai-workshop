export type EmbeddingModel = "text-embedding-ada-002";
// export type ChatModel = "gpt-4-1106-preview" | "gpt-3.5-turbo" | "gpt-3.5-turbo-16k";
export type ChatModel = "gpt-4-0613" | "gpt-35-turbo-0613";
export type MessageRole = "system" | "user" | "assistant" | "function";

interface MessageBase {
  role: MessageRole;
  content: string;
}

export interface UserMessage extends MessageBase {
  name?: string;
}
export interface SystemMessage extends MessageBase {
  name?: string;
}

export interface AssistantMessage extends MessageBase {
  name?: string;
  function_call: Function;
}

export type Message = SystemMessage | UserMessage | AssistantMessage;

interface Function {
  // The name of the function to call.
  name: string;
  // The arguments to call the function with, as generated by the model in JSON format.
  // Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema.
  // Validate the arguments in your code before calling your function.
  arguments: string;
}

export class FunctionDefinition {
  name: string;
  description?: string;

  /**
   * The parameters the functions accepts, described as a JSON Schema object.
   * See the [guide for
   * examples](https://platform.openai.com/docs/guides/gpt/function-calling),
   * and the [JSON Schema
   * reference](https://json-schema.org/understanding-json-schema/) for
   * documentation about the format.
   */
  parameters?: string;
}

export interface ChatCompletionRequest {
  messages: Message[];
  temperature?: number;
  top_p?: number;
  n?: number;
  stream?: boolean;
  stop?: string | string[];
  max_tokens?: number;
  // -2.0 to 2.0
  presence_penalty?: number;

  // -2.0 to 2.0
  frequency_penalty?: number;

  logit_bias?: Record<string, number>;

  // A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
  user?: string;

  /**
   * Controls which (if any) function is called by the model. none means the model
   * will not call a function and instead generates a message. auto means the model
   * can pick between generating a message or calling a function. Specifying a
   * particular function via {"type: "function", "function": {"name": "my_function"}}
   * forces the model to call that function.
   *
   * none is the default when no functions are present. auto is the default if
   * functions are present.
   */
  function_call?: "auto" | "none" | { name: string };

  functions?: FunctionDefinition[];
}

export interface ChatCompletionResponse {
  id: string;
  object: "chat.completion";
  created: number;
  model: ChatModel;
  // This fingerprint represents the backend configuration that the model runs with.
  // Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.
  system_fingerprint: string;
  choices: {
    index: number;
    message: Message;
    /**
     * - stop: API returned complete message, or a message terminated by one of
     *   the stop sequences provided via the stop parameter
     * - length: Incomplete model output due to max_tokens parameter or token
     *   limit
     * - function_call: The model decided to call a function
     * - content_filter: Omitted content due to a flag from our content filters
     * - null: API response still in progress or incomplete
     */
    finish_reason: "stop" | "length" | "function_call" | "content_filter" | null;
  }[];
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}
